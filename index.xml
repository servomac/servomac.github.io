<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Another Machine Learning hyped blog</title>
    <link>https://servomac.github.io/index.xml</link>
    <description>Recent content on Another Machine Learning hyped blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 17 Mar 2018 12:26:00 +0100</lastBuildDate>
    <atom:link href="https://servomac.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Books readed in 2018</title>
      <link>https://servomac.github.io/blog/books-readed-in-2018/</link>
      <pubDate>Sat, 17 Mar 2018 12:26:00 +0100</pubDate>
      
      <guid>https://servomac.github.io/blog/books-readed-in-2018/</guid>
      <description>

&lt;h3 id=&#34;march&#34;&gt;March&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Ten-Peor-Coche-Vecino-VIVA/dp/8483566567&#34;&gt;&lt;em&gt;Ten peor coche que tu vecino&lt;/em&gt;&lt;/a&gt;, by Luis Pita&lt;/p&gt;

&lt;h3 id=&#34;february&#34;&gt;February&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Rich-Dad-Poor-Teach-Middle/dp/1612680011&#34;&gt;&lt;em&gt;Poor dad, rich dad&lt;/em&gt;&lt;/a&gt;, by Robert T. Kiyosaki&lt;/p&gt;

&lt;h3 id=&#34;january&#34;&gt;January&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Hacking-Growth-Morgan-Brown/dp/0753545373&#34;&gt;&lt;em&gt;Hacking Growth&lt;/em&gt;&lt;/a&gt;, by Sean Ellis &amp;amp; Morgan Brown&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Books readed in 2017</title>
      <link>https://servomac.github.io/blog/books-readed-in-2017/</link>
      <pubDate>Fri, 01 Sep 2017 21:23:22 +0200</pubDate>
      
      <guid>https://servomac.github.io/blog/books-readed-in-2017/</guid>
      <description>

&lt;h3 id=&#34;december&#34;&gt;December&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Cartes-Caubet-Rossell%C3%B3-Cun%C3%A9-Josep/dp/8415076576&#34;&gt;&lt;em&gt;Cartes des de Caubet&lt;/em&gt;&lt;/a&gt;, by Josep-Joan Rosselló Cuní&lt;/p&gt;

&lt;h3 id=&#34;november&#34;&gt;November&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Clean-Architecture-Craftsmans-Software-Structure/dp/0134494164&#34;&gt;&lt;em&gt;Clean Architecture&lt;/em&gt;&lt;/a&gt;, by Robert C. Martin&lt;/p&gt;

&lt;h3 id=&#34;october&#34;&gt;October&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Matchmakers-New-Economics-Multisided-Platforms/dp/1633691721&#34;&gt;&lt;em&gt;Matchmakers: The new economics of multisided platforms&lt;/em&gt;&lt;/a&gt;, by David S. Evans &amp;amp; Richard Schmalensee&lt;/p&gt;

&lt;h3 id=&#34;september&#34;&gt;September&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Irresistible-APIs-Designing-that-developers/dp/1617292559&#34;&gt;&lt;em&gt;Irresistible APIs&lt;/em&gt;&lt;/a&gt;, by Kirsten L. Hunter&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Homo-Deus-Yuval-Noah-Harari/dp/1784703931&#34;&gt;&lt;em&gt;Homo Deus&lt;/em&gt;&lt;/a&gt;, by Yuval Noah Harari&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Confessions-Pricing-Man-Affects-Everything-ebook/dp/B016XMVQA6&#34;&gt;&lt;em&gt;Confesions of the Pricing Man&lt;/em&gt;&lt;/a&gt;, by Hermann Simon&lt;/p&gt;

&lt;h3 id=&#34;august&#34;&gt;August&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920048992.do&#34;&gt;&lt;em&gt;Practical Statistics for Data Scientists&lt;/em&gt;&lt;/a&gt;, by Peter Bruce &amp;amp; Andrew Bruce&lt;/p&gt;

&lt;h3 id=&#34;july&#34;&gt;July&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Headstrong-Women-Changed-Science-World/dp/0553446797&#34;&gt;&lt;em&gt;Headstrong: 52 Women Who Changed Science-and the World&lt;/em&gt;&lt;/a&gt;, by Rachel Swaby&lt;/p&gt;

&lt;h3 id=&#34;june&#34;&gt;June&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Antigone_(Sophocles_play)&#34;&gt;&lt;em&gt;Antigone&lt;/em&gt;&lt;/a&gt;, by Sophocles&lt;/p&gt;

&lt;h3 id=&#34;may&#34;&gt;May&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Why_Nations_Fail&#34;&gt;&lt;em&gt;Why Nations Fail&lt;/em&gt;&lt;/a&gt;, by Daron Acemoglu &amp;amp; James A. Robinson&lt;/p&gt;

&lt;h3 id=&#34;april&#34;&gt;April&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.amazon.es/Predictive-Analytics-Power-Predict-Click/dp/1119145678&#34;&gt;&lt;em&gt;Predictive Analytics&lt;/em&gt;&lt;/a&gt;, by Eric Siegel&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning from imbalanced data</title>
      <link>https://servomac.github.io/blog/learning-from-imbalanced-data/</link>
      <pubDate>Sun, 05 Feb 2017 18:00:00 +0200</pubDate>
      
      <guid>https://servomac.github.io/blog/learning-from-imbalanced-data/</guid>
      <description>

&lt;h2 id=&#34;what-is-an-imbalanced-dataset&#34;&gt;What is an imbalanced dataset?&lt;/h2&gt;

&lt;p&gt;Most algorithms for machine learning assume that the training set data classes are balanced. But a lot of real world scenarios present classification problems where the involved classes aren&amp;rsquo;t equally represented: one class is composed by a large number of instances, while the other contains a small number of examples. In this case, the learning algorithm may have difficulties learning the concepts representing the minority class.&lt;/p&gt;

&lt;p&gt;Imagine that you are trying to predict the probability that a client leaves your platform (churn analysis). If your product have a good retention ratio, after extracting the dataset from the historical data, your users dataset will contain a lot of examples of people renewing and a minority cancelling their service pack.&lt;/p&gt;

&lt;p&gt;Detection of fraudulent transactions, medical diagnosis and detection of oil spills in satellital images are other examples of problems where one of the classes in the dataset is over-represented.&lt;/p&gt;

&lt;p&gt;This kind of datasets usually result in the &lt;strong&gt;accuracy paradox&lt;/strong&gt;. Imagine a binary
classification algorithm, trying to maximize the predictive accuracy of the model,
learning from an imbalanced dataset where 99% of the data is from one class:
it could construct a trivial classifier that labels every new case as the majority class, ¡achieving a 99% accuracy! This is especially dangerous when the cost of misclassification of the minority class is high.&lt;/p&gt;

&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;

&lt;p&gt;I will construct an artificial imbalanced dataset using &lt;code&gt;make_moons&lt;/code&gt;, a &lt;code&gt;sklearn&lt;/code&gt;
method to generate a dataset with two classes distributed as interleaving half circles.
I also used &lt;code&gt;make_imbalance&lt;/code&gt; method from &lt;code&gt;imbalanced-learn&lt;/code&gt; library to skew the
dataset and create a majority / minority class with a minority class representing
the 10% percent of the dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_moons
from imblearn.datasets import make_imbalance

N_SAMPLES = 1500
MINORITY_RATIO = 0.1

X_balanced, y_balanced = make_moons(n_samples=N_SAMPLES,
                                    shuffle=True,
                                    noise=0.3,
                                    random_state=10)
X, y = make_imbalance(X_balanced,
                      y_balanced,
                      ratio=MINORITY_RATIO,
                      min_c_=1)

f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, sharex=True)

ax1.set_title(&amp;quot;Balanced&amp;quot;)
ax1.scatter(X_balanced[:, 0], X_balanced[:, 1], marker=&#39;o&#39;, c=y_balanced)

ax2.set_title(&amp;quot;Imbalanced&amp;quot;)
ax2.scatter(X[:, 0], X[:, 1], marker=&#39;o&#39;, c=y)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;measuring-the-model-performance&#34;&gt;Measuring the model performance&lt;/h2&gt;

&lt;p&gt;A classifier in supervised learning is tipically evaluated using a confusion matrix, with columns representing the predicted class and rows representing the actual class. I will use examples of binary classification, but this is generalizable to any number of classes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://servomac.github.io/img/confusion-matrix.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the confusion matrix, the &lt;em&gt;True Negatives&lt;/em&gt; (TN) represent the number of instances of the negative class correctly classified, while &lt;em&gt;False Positives&lt;/em&gt; (FP) are the negative instances classified as positive. &lt;em&gt;False Negatives&lt;/em&gt; (FN) are those example instances of positive class in the test data that are labeled by the predictor as negative; and &lt;em&gt;True Positives&lt;/em&gt; (TP) represent those instances of the positive class correctly classified.&lt;/p&gt;

&lt;p&gt;The accuracy score is an evaluation metric that defines the predictive accuracy of the model as &lt;code&gt;(TP + TN) / (TP + FP + TN + FN)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, this accuracy score is not enought to describe the accuracy of a model when confronting imbalanced datasets, or when the cost of misclassification differs between classes.&lt;/p&gt;

&lt;p&gt;A good example of this is the &lt;a href=&#34;http://odds.cs.stonybrook.edu/mammography-dataset/&#34;&gt;Mammography dataset&lt;/a&gt;, a skewed data set with a total of 11183 samples with 260 representing calcifications, the positive class. It&amp;rsquo;s also important to improve the sensity of the positive class in the resultant model, due to the nature of the problem.&lt;/p&gt;

&lt;p&gt;To avoid the problems related with accuracy score when dealing with imbalanced data, Area Under the ROC Curve (&lt;a href=&#34;http://www.dataschool.io/roc-curves-and-auc-explained/&#34;&gt;AUC&lt;/a&gt;) has emerged as a popular choice for model performance measurement (&lt;a href=&#34;http://users.dsic.upv.es/~flip/papers/sigkdd2004.pdf&#34;&gt;Ferri et al, 2004&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;approaches-to-the-problem&#34;&gt;Approaches to the problem&lt;/h2&gt;

&lt;p&gt;The methods proposed to deal with the class imbalance problem could be divided in two main categories: those centered in modifying the learner (algorithm / cost function based) and those centered in balancing the training dataset (resampling based).&lt;/p&gt;

&lt;p&gt;The sampling methods are centered in modifying the data distribution through sampling algorithms that create a balanced dataset from our original imbalanced dataset. This allows to the algorithm to improve the precision of the minority class predictions. There are oversampling (expand the examples of the minority class) and undersampling (shrink the majority class, with a possibility of losing data) methods.&lt;/p&gt;

&lt;p&gt;Instead, the methods centered in the algorithm modify it to use cost-sensitive methods that take into account the cost of misclassifying the minority class.&lt;/p&gt;

&lt;h3 id=&#34;resampling-methods-in-python-imbalanced-learn&#34;&gt;Resampling methods in Python: Imbalanced learn&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/scikit-learn-contrib/imbalanced-learn&#34;&gt;imbalanced-learn&lt;/a&gt; is a Python module implementing a lot of those resampling methods. It&amp;rsquo;s part of the scikit learn contrib projects, and easily integrable with your scikit learn code. Try it, it&amp;rsquo;s amazing!&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Remember, creating models from imbalanced data could be a difficult task. You should use evaluation methods for your models resulting from imbalanced data that take into account those particularities. Select correctly the learning algorithm, and evaluate using resampling methods for the training dataset. Let me know your experiences!&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;H. He and E. A. Garcia, &amp;ldquo;Learning from Imbalanced Data&amp;rdquo;. IEEE Trans. Knowledge and Data Engineering, vol. 21, issue 9, pp. 1263-­‐1284, 2009.&lt;/li&gt;
&lt;li&gt;G. Lemaitre and F. Nogueira and C. K. Aridas. &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/1609.06570&#34;&gt;Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning&lt;/a&gt;. CoRR, 2016.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>3 months study guide</title>
      <link>https://servomac.github.io/blog/3-months-study-guide/</link>
      <pubDate>Mon, 17 Oct 2016 18:20:37 +0200</pubDate>
      
      <guid>https://servomac.github.io/blog/3-months-study-guide/</guid>
      <description>

&lt;p&gt;During the next months I will be struggling with a career path shift (i really prefer to say that it&amp;rsquo;s a career &lt;em&gt;fork&lt;/em&gt;). Currently I&amp;rsquo;m working as a devops engineer, but &lt;strong&gt;I&amp;rsquo;m pivoting to a data science / machine learning&lt;/strong&gt; developer position. I have always been interested in a wide range of aspects of the computer science, being more a generalist than a specialist (even that market seems to favour specialists in a concrete level of the stack, and I have specialized in distributed systems and web reliability). At the moment, data science seems an emerging field with a lot of impact and new challenges that really attracts me. I have been presented with the opportunity of being part of a new business intelligence team at my current company, and I&amp;rsquo;m excited with the possibility of learning new things and to apply my analytic mindset from this new position.&lt;/p&gt;

&lt;p&gt;Despite being involved in some small projects related to machine learning, such as automatic text classification and other kind of predictive modelling, I&amp;rsquo;m new to this field, so I will be working hard to get in shape with its knowledge corpus. I have always been interested in the subject, and it&amp;rsquo;s a good excuse to go back to study. I have never stopped learning, but not in a &lt;em&gt;structured&lt;/em&gt; way.&lt;/p&gt;

&lt;p&gt;This blog entry is an action plan; a sort of guide for my next months of study, that will be for sure reevaluated during the way. My objective is to become an advanced beginner (Dreyfus has been here) in the machine learning and data science fields.&lt;/p&gt;

&lt;h3 id=&#34;october&#34;&gt;October&lt;/h3&gt;

&lt;p&gt;I have been taking the Udacity&amp;rsquo;s free course &lt;a href=&#34;https://www.udacity.com/course/intro-to-machine-learning--ud120&#34;&gt;Intro to Machine Learning&lt;/a&gt;. I really enjoyed the topics of the course, despite not having finished it (76% completed, starting the PCA lesson). It starts from the bottom. It is not extremely technical and formal with the mathematical foundation of the algorithms presented. Even though, it gives a good overview of the machine learning landscape. I strongly recommend it to the field newcomers. &lt;strong&gt;My first objective during this month is to finish it&lt;/strong&gt;. I&amp;rsquo;m estimating to finish the course in 10 hours.&lt;/p&gt;

&lt;p&gt;Another Udacity&amp;rsquo;s course that I will do during this month is &lt;a href=&#34;https://www.udacity.com/course/intro-to-data-analysis--ud170&#34;&gt;Intro to Data Analysis&lt;/a&gt;. It seems a much shorter course than Intro to Machine Learning, with an introductory aim. Python is my main language, so the pandas and numpy lessons will be useful, as well as the data science introduction. I estimate a dedication of 25 hours.&lt;/p&gt;

&lt;p&gt;One of my biggest weaknesses is my lack of statistic knowledge and intuition. I have purchased a physical copy of &lt;a href=&#34;http://www.openintro.net/stat/textbook.php?stat_book=os&#34;&gt;OpenIntro Statistics&lt;/a&gt; (it&amp;rsquo;s available as a PDF for free) and I will be reading and studying the book, doing the exercises to refresh the knowledge acquired during my computer science degree. The book has 8 lessons (introduction to data, probability, distributions of random variables, foundations of inference, inference for numerical data, inference for categorical data, introduction to linear regression, multiple and logistic regression). During this month I want to &lt;strong&gt;finish the 2 first lessons&lt;/strong&gt;, with their respective exercises. This cannot be just a diagonal lecture, for me it requires a careful study; I estimate it on 8 hours.&lt;/p&gt;

&lt;p&gt;Also, I want to read the book &lt;strong&gt;Clean Code&lt;/strong&gt;, and I&amp;rsquo;m close to ending &lt;a href=&#34;https://leanpub.com/artofdatascience&#34;&gt;&lt;strong&gt;The Art of Data Science&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If my estimations are correct (and obviously they will not be; it&amp;rsquo;s just a challenge -i will review the real dedicated time at the end of the month-), I will be studying during 43 hours (not including the lecture time). I&amp;rsquo;m starting to write this post on October 17th; this is a challenging 3 hours of dedication a day. Obviously this is impossible; I&amp;rsquo;m working full time at the same time than doing all this self study. I have 5 entire days to dedicate to study (weekends and a holiday day), so I will try to dedicate 7 hours each of these days and 1 hour the normal days. Seems to me like a crazy enterprise. If your dreams don&amp;rsquo;t scare you, they aren&amp;rsquo;t big enough.&lt;/p&gt;

&lt;h3 id=&#34;november&#34;&gt;November&lt;/h3&gt;

&lt;p&gt;The second month I will continue centered on starting to build a good foundational statistics background. &lt;strong&gt;I will study the pending lessons from OpenIntro Statistics&lt;/strong&gt;. Moreover, I want to do the Udacity&amp;rsquo;s course &lt;a href=&#34;https://www.udacity.com/course/intro-to-inferential-statistics--ud201&#34;&gt;&lt;strong&gt;Intro to Inferential Statistics&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have been interested in the Go language for a few months. I want to learn the basics of Go, doing the tutorial &lt;a href=&#34;https://tour.golang.org/&#34;&gt;A tour of Go&lt;/a&gt;. A good intersection of learning Go with my main objective of increasing my competence in machine learning could be implementing some basic machine learning algorithms. A basic linear regression and decision trees could be a good starting point (learn by doing!).&lt;/p&gt;

&lt;p&gt;I want to read the book &lt;a href=&#34;https://www.amazon.es/Data-Science-Business-data-analytic-thinking/dp/1449361323&#34;&gt;&lt;strong&gt;Data Science for business&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;december&#34;&gt;December&lt;/h3&gt;

&lt;p&gt;During December I want to start the Coursera&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Machine Learning&lt;/a&gt; course by Andrew Ng. Maybe it overlaps a little with other online courses, but a lot of people recommend it so I think it is a good ending to my introductory months.&lt;/p&gt;

&lt;p&gt;Also, I want to continue implementing some basic machine learning algorithms with Go. During the coursera&amp;rsquo;s lessons, I will challenge myself to code some of the algorithms while reviewing them at the Andrew Ng&amp;rsquo;s course.&lt;/p&gt;

&lt;p&gt;The book to read this month will be &lt;a href=&#34;https://www.amazon.com/Machine-Learning-Hackers-Drew-Conway/dp/1449303714/&#34;&gt;&lt;strong&gt;Machine Learning for Hackers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;future-work&#34;&gt;Future work&lt;/h2&gt;

&lt;p&gt;At the end of the year I will review my advances and feelings after this study sprint. During January I will start working full time as a machine learning developer in the new data science team at my current workplace, so I will be challenged with a lot of practical data analysis tasks and the development of production ready predictive models.&lt;/p&gt;

&lt;p&gt;On the self study part, I need to continue becoming confident with the theoretical foundations of machine learning as well as expanding my practical knowledge in the data science field. I want to do the Standford &lt;a href=&#34;https://see.stanford.edu/Course/CS229&#34;&gt;CS229 course&lt;/a&gt; and Machine Learning udacity&amp;rsquo;s &lt;a href=&#34;https://www.udacity.com/course/machine-learning--ud262&#34;&gt;course 262&lt;/a&gt; by Georgia Tech. Furthermore, I want to start with the study of neural nets (the hype of the deep learning is not to be ignored). Another important objective in the future is to start participating in &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; competitions and building a public portfolio.&lt;/p&gt;

&lt;p&gt;But the most important current and future task is to generate good study habits, compatible with a happy social life, a passionate work and a healthy leisure, to continue this knowledge marathon after this initial trip.&lt;/p&gt;

&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.quora.com/How-can-I-become-a-data-scientist-1&#34;&gt;Quora: How can i become a data scientist?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=12713056&#34;&gt;Ask HN: How to get started with Machine Learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ZuzooVn/machine-learning-for-software-engineers&#34;&gt;Machine learning for Software Engineers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Books readed in 2016</title>
      <link>https://servomac.github.io/blog/books-readed-in-2016/</link>
      <pubDate>Tue, 04 Oct 2016 19:33:11 +0200</pubDate>
      
      <guid>https://servomac.github.io/blog/books-readed-in-2016/</guid>
      <description>

&lt;h3 id=&#34;december&#34;&gt;December&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Data Science for Business&lt;/em&gt;, by Foster Provost and Tom Fawcett&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;november&#34;&gt;November&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Refactoring&lt;/em&gt;, by Martin Fowler&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;october&#34;&gt;October&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Clean Code&lt;/em&gt;, by Robert C. Martin&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Art of Data Science&lt;/em&gt;, by Roger D. Peng and Elizabeth Matsui&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;september&#34;&gt;September&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sapiens&lt;/em&gt;, by Yuval Noah Harari&lt;/li&gt;
&lt;li&gt;&lt;em&gt;How not to be wrong&lt;/em&gt;, by Jordon Ellenberg&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;august&#34;&gt;August&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The second machine age&lt;/em&gt;, by Erik Brynjolfsson and Andrew McAfee&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Doing Data Science&lt;/em&gt;, by Cathy O&amp;rsquo;Neil and Rachel Schutt&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>https://servomac.github.io/fixed/about/</link>
      <pubDate>Tue, 04 Oct 2016 19:21:32 +0200</pubDate>
      
      <guid>https://servomac.github.io/fixed/about/</guid>
      <description>&lt;p&gt;My name is Toni Pizà. I&amp;rsquo;m a developer with a wide range of interests, mainly involved in algorithmics, web architecture, distributed systems and machine learning. I have a bachelor&amp;rsquo;s degree in Computer Science by the University of the Balearic Islands (UIB), and I have coursed a master degree in Computing by the Polytechnic University of Catalonia (UPC), but i haven&amp;rsquo;t read my master thesis.&lt;/p&gt;

&lt;p&gt;I worked as a backend developer and devops engineer. Currently i&amp;rsquo;m starting a new path in the data science field, with special emphasis in machine learning.&lt;/p&gt;

&lt;p&gt;This blog is an attempt to improve my written expression in english. Also, putting down my ideas is a good way to structurate them, and the public exposure can be useful to recolect feedback of people with more experience than me. I will appreciate any comment, correction and improvement to the ideas exposed in this blog.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>