<!DOCTYPE html>















<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  
  <title>Learning from imbalanced data - servo&#39;s blog</title>

  
  
  <meta name="description" content="What is an imbalanced dataset? Most algorithms for machine learning assume that the training set data classes are balanced. But a lot of real world scenarios present classification problems where the involved classes aren&rsquo;t equally represented: one class is composed by a large number of instances, while the other contains a small number of examples. In this case, the learning algorithm may have difficulties learning the concepts representing the minority class." />
  <meta name="author" content="" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://servomac.github.io/app.min.css" />

  
  <link rel="preload stylesheet" as="style" href="https://servomac.github.io/an-old-hope.min.css" />
  <script
    defer
    src="https://servomac.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  <link rel="preload" as="image" href="https://servomac.github.io/theme.png" />

  
  <link rel="preload" as="image" href="https://servomac.github.io/twitter.svg" />
  
  <link rel="preload" as="image" href="https://servomac.github.io/github.svg" />
  

  
  <link rel="icon" href="https://servomac.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://servomac.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.101.0" />

  
  

  
  
  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-86068014-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  
  
  
  <meta property="og:title" content="Learning from imbalanced data" />
<meta property="og:description" content="What is an imbalanced dataset? Most algorithms for machine learning assume that the training set data classes are balanced. But a lot of real world scenarios present classification problems where the involved classes aren&rsquo;t equally represented: one class is composed by a large number of instances, while the other contains a small number of examples. In this case, the learning algorithm may have difficulties learning the concepts representing the minority class." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://servomac.github.io/blog/learning-from-imbalanced-data/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2017-02-05T18:00:00+02:00" />
<meta property="article:modified_time" content="2017-02-05T18:00:00+02:00" />


  
  <meta itemprop="name" content="Learning from imbalanced data">
<meta itemprop="description" content="What is an imbalanced dataset? Most algorithms for machine learning assume that the training set data classes are balanced. But a lot of real world scenarios present classification problems where the involved classes aren&rsquo;t equally represented: one class is composed by a large number of instances, while the other contains a small number of examples. In this case, the learning algorithm may have difficulties learning the concepts representing the minority class."><meta itemprop="datePublished" content="2017-02-05T18:00:00+02:00" />
<meta itemprop="dateModified" content="2017-02-05T18:00:00+02:00" />
<meta itemprop="wordCount" content="844">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Learning from imbalanced data"/>
<meta name="twitter:description" content="What is an imbalanced dataset? Most algorithms for machine learning assume that the training set data classes are balanced. But a lot of real world scenarios present classification problems where the involved classes aren&rsquo;t equally represented: one class is composed by a large number of instances, while the other contains a small number of examples. In this case, the learning algorithm may have difficulties learning the concepts representing the minority class."/>

  
  
</head>


  <body class="not-ready" data-menu="true">
    <header class="header">
  
  <p class="logo">
    <a class="site-name" href="https://servomac.github.io/">servo&#39;s blog</a><a class="btn-dark"></a>
  </p>
  

  <script>
    let bodyClx = document.body.classList;
    let btnDark = document.querySelector('.btn-dark');
    let sysDark = window.matchMedia('(prefers-color-scheme: dark)');
    let darkVal = localStorage.getItem('dark');

    let setDark = (isDark) => {
      bodyClx[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark ? 'yes' : 'no');
    };

    setDark(darkVal ? darkVal === 'yes' : sysDark.matches);
    requestAnimationFrame(() => bodyClx.remove('not-ready'));

    btnDark.addEventListener('click', () => setDark(!bodyClx.contains('dark')));
    sysDark.addEventListener('change', (event) => setDark(event.matches));
  </script>

  
  
  <nav class="menu">
    
    <a class="" href="/about/">About</a>
    
  </nav>
  

  
  <nav class="social">
    
    <a
      class="twitter"
      style="--url: url(./twitter.svg)"
      href="https://twitter.com/pushsink"
      target="_blank"
    ></a>
    
    <a
      class="github"
      style="--url: url(./github.svg)"
      href="https://github.com/servomac"
      target="_blank"
    ></a>
    
  </nav>
  
</header>


    <main class="main">

<article class="post-single">
  <header class="post-title">
    <p>
      <time>Feb 5, 2017</time>
      
    </p>
    <h1>Learning from imbalanced data</h1>
  </header>
  <section class="post-content"><h2 id="what-is-an-imbalanced-dataset">What is an imbalanced dataset?</h2>
<p>Most algorithms for machine learning assume that the training set data classes are balanced. But a lot of real world scenarios present classification problems where the involved classes aren&rsquo;t equally represented: one class is composed by a large number of instances, while the other contains a small number of examples. In this case, the learning algorithm may have difficulties learning the concepts representing the minority class.</p>
<p>Imagine that you are trying to predict the probability that a client leaves your platform (churn analysis). If your product have a good retention ratio, after extracting the dataset from the historical data, your users dataset will contain a lot of examples of people renewing and a minority cancelling their service pack.</p>
<p>Detection of fraudulent transactions, medical diagnosis and detection of oil spills in satellital images are other examples of problems where one of the classes in the dataset is over-represented.</p>
<p>This kind of datasets usually result in the <strong>accuracy paradox</strong>. Imagine a binary
classification algorithm, trying to maximize the predictive accuracy of the model,
learning from an imbalanced dataset where 99% of the data is from one class:
it could construct a trivial classifier that labels every new case as the majority class, ¡achieving a 99% accuracy! This is especially dangerous when the cost of misclassification of the minority class is high.</p>
<h3 id="example">Example</h3>
<p>I will construct an artificial imbalanced dataset using <code>make_moons</code>, a <code>sklearn</code>
method to generate a dataset with two classes distributed as interleaving half circles.
I also used <code>make_imbalance</code> method from <code>imbalanced-learn</code> library to skew the
dataset and create a majority / minority class with a minority class representing
the 10% percent of the dataset:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_moons
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> imblearn.datasets <span style="color:#f92672">import</span> make_imbalance
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N_SAMPLES <span style="color:#f92672">=</span> <span style="color:#ae81ff">1500</span>
</span></span><span style="display:flex;"><span>MINORITY_RATIO <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_balanced, y_balanced <span style="color:#f92672">=</span> make_moons(n_samples<span style="color:#f92672">=</span>N_SAMPLES,
</span></span><span style="display:flex;"><span>                                    shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                    noise<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>,
</span></span><span style="display:flex;"><span>                                    random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> make_imbalance(X_balanced,
</span></span><span style="display:flex;"><span>                      y_balanced,
</span></span><span style="display:flex;"><span>                      ratio<span style="color:#f92672">=</span>MINORITY_RATIO,
</span></span><span style="display:flex;"><span>                      min_c_<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>f, (ax1, ax2) <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, sharey<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, sharex<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Balanced&#34;</span>)
</span></span><span style="display:flex;"><span>ax1<span style="color:#f92672">.</span>scatter(X_balanced[:, <span style="color:#ae81ff">0</span>], X_balanced[:, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, c<span style="color:#f92672">=</span>y_balanced)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Imbalanced&#34;</span>)
</span></span><span style="display:flex;"><span>ax2<span style="color:#f92672">.</span>scatter(X[:, <span style="color:#ae81ff">0</span>], X[:, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, c<span style="color:#f92672">=</span>y)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h2 id="measuring-the-model-performance">Measuring the model performance</h2>
<p>A classifier in supervised learning is tipically evaluated using a confusion matrix, with columns representing the predicted class and rows representing the actual class. I will use examples of binary classification, but this is generalizable to any number of classes.</p>
<p><img src="/img/confusion-matrix.png" alt=""></p>
<p>In the confusion matrix, the <em>True Negatives</em> (TN) represent the number of instances of the negative class correctly classified, while <em>False Positives</em> (FP) are the negative instances classified as positive. <em>False Negatives</em> (FN) are those example instances of positive class in the test data that are labeled by the predictor as negative; and <em>True Positives</em> (TP) represent those instances of the positive class correctly classified.</p>
<p>The accuracy score is an evaluation metric that defines the predictive accuracy of the model as <code>(TP + TN) / (TP + FP + TN + FN)</code>.</p>
<p>Unfortunately, this accuracy score is not enought to describe the accuracy of a model when confronting imbalanced datasets, or when the cost of misclassification differs between classes.</p>
<p>A good example of this is the <a href="http://odds.cs.stonybrook.edu/mammography-dataset/">Mammography dataset</a>, a skewed data set with a total of 11183 samples with 260 representing calcifications, the positive class. It&rsquo;s also important to improve the sensity of the positive class in the resultant model, due to the nature of the problem.</p>
<p>To avoid the problems related with accuracy score when dealing with imbalanced data, Area Under the ROC Curve (<a href="http://www.dataschool.io/roc-curves-and-auc-explained/">AUC</a>) has emerged as a popular choice for model performance measurement (<a href="http://users.dsic.upv.es/~flip/papers/sigkdd2004.pdf">Ferri et al, 2004</a>).</p>
<h2 id="approaches-to-the-problem">Approaches to the problem</h2>
<p>The methods proposed to deal with the class imbalance problem could be divided in two main categories: those centered in modifying the learner (algorithm / cost function based) and those centered in balancing the training dataset (resampling based).</p>
<p>The sampling methods are centered in modifying the data distribution through sampling algorithms that create a balanced dataset from our original imbalanced dataset. This allows to the algorithm to improve the precision of the minority class predictions. There are oversampling (expand the examples of the minority class) and undersampling (shrink the majority class, with a possibility of losing data) methods.</p>
<p>Instead, the methods centered in the algorithm modify it to use cost-sensitive methods that take into account the cost of misclassifying the minority class.</p>
<h3 id="resampling-methods-in-python-imbalanced-learn">Resampling methods in Python: Imbalanced learn</h3>
<p><a href="https://github.com/scikit-learn-contrib/imbalanced-learn">imbalanced-learn</a> is a Python module implementing a lot of those resampling methods. It&rsquo;s part of the scikit learn contrib projects, and easily integrable with your scikit learn code. Try it, it&rsquo;s amazing!</p>
<h2 id="conclusions">Conclusions</h2>
<p>Remember, creating models from imbalanced data could be a difficult task. You should use evaluation methods for your models resulting from imbalanced data that take into account those particularities. Select correctly the learning algorithm, and evaluate using resampling methods for the training dataset. Let me know your experiences!</p>
<h1 id="references">References</h1>
<ul>
<li>H. He and E. A. Garcia, &ldquo;Learning from Imbalanced Data&rdquo;. IEEE Trans. Knowledge and Data Engineering, vol. 21, issue 9, pp. 1263-­‐1284, 2009.</li>
<li>G. Lemaitre and F. Nogueira and C. K. Aridas. &ldquo;<a href="https://arxiv.org/abs/1609.06570">Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning</a>. CoRR, 2016.</li>
</ul>
</section>

  
  

  
  
  
  <nav class="post-nav">
    
    <a class="prev" href="https://servomac.github.io/blog/books-readed-in-2017/"><span>←</span><span>Books readed in 2017</span></a>
     
    <a class="next" href="https://servomac.github.io/blog/3-months-study-guide/"><span>3 months study guide</span><span>→</span></a>
    
  </nav>
  

  
  
</article>

</main>

    <footer class="footer">
  <p>&copy; 2022 <a href="https://servomac.github.io/">servo&#39;s blog</a></p>
  <p>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>️</p>
  <p>
    <a href="https://github.com/nanxiaobei/hugo-paper" rel="noopener" target="_blank">Paper 5.1</a>
  </p>
</footer>

  </body>
</html>
